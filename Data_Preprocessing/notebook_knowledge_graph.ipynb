{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kunmi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/kunmi/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/kunmi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/kunmi/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kunmi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/kunmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kunmi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/kunmi/miniforge3/lib/python3.9/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#### IMPORT LIBRARIES\n",
    "#\n",
    "# \n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from IPython.display import display\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, dependencygraph\n",
    "from nltk import RegexpParser\n",
    "from nltk import Tree\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "pd.set_option ('display.max_colwidth', None)\n",
    "pd.set_option ('display.max_columns', None)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "nltk.download ('punkt')\n",
    "nltk.download ('stopwords')\n",
    "nltk.download ('wordnet')\n",
    "import warnings\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel ('../Data/preprocessed/NG_ELECTION_TWEETS_LABELLED.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @AtedoPeterside: Nobody \"says\" it better than our GoAmbassador @mrmacaronii. Take matters into your own hands by acquiring your PVC now.…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @1dernet: Getting Our PVC And Registering To Vote Come 2023 Is The Nonviolent Part To Take.\\n#GetYourPVC \\n@DeleFarotimi \\n@Tsngcampaign ht…</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment  \\\n",
       "0  Positive   \n",
       "1   Neutral   \n",
       "\n",
       "                                                                                                                                              text  \n",
       "0     RT @AtedoPeterside: Nobody \"says\" it better than our GoAmbassador @mrmacaronii. Take matters into your own hands by acquiring your PVC now.…  \n",
       "1  RT @1dernet: Getting Our PVC And Registering To Vote Come 2023 Is The Nonviolent Part To Take.\\n#GetYourPVC \\n@DeleFarotimi \\n@Tsngcampaign ht…  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "\n",
    "  doc = nlp(sent)\n",
    "\n",
    "  # Matcher class object \n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  #define the pattern \n",
    "  pattern = [\n",
    "    {'DEP':'ROOT'}, \n",
    "    {'DEP':'prep','OP':\"?\"},\n",
    "    {'DEP':'agent','OP':\"?\"},  \n",
    "    {'POS':'ADJ','OP':\"?\"}\n",
    "  ] \n",
    "\n",
    "  matcher.add(\"matching_1\", [pattern]) \n",
    "\n",
    "  matches = matcher (doc)\n",
    "  k = len(matches) - 1\n",
    "\n",
    "  span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "  return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cleaner (tweet):\n",
    "  soup    = BeautifulSoup (tweet, 'lxml') # removing HTML entities such as '&amp', '&quot', '&gt'; lxml is the html parser\n",
    "  souped  = soup.get_text ()\n",
    "  link_re = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "  re_ = re.sub (link_re, \"\", souped)\n",
    "  re_ = re.sub (r\"#\\S*\", \"\", re_)       # Replace all hashtags with an empty string\n",
    "  re_ = re.sub (\"RT @\\S*\", \" \", re_)\n",
    "  re_ = re.sub (r\"[^A-Za-z]+\", \" \", re_) # sustituting any non-alphabetic character that repeats one or more time\n",
    "\n",
    "  tokens     = nltk.word_tokenize (re_)\n",
    "  lower_case = [t.lower () for t in tokens]\n",
    "\n",
    "  stop_words      = set (stopwords.words ('english'))\n",
    "  filtered_result = list (filter (lambda l: l not in stop_words, lower_case))\n",
    "\n",
    "  wordnet_lemmatizer = WordNetLemmatizer ()\n",
    "  lemmas             = [wordnet_lemmatizer.lemmatize (t) for t in filtered_result]\n",
    "\n",
    "  return lower_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[30m██████████\u001b[0m| 11659/11659 [00:10<00:00, 1130.54it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas ()\n",
    "\n",
    "for row, col  in tqdm(df['text'].iteritems (), total=df.shape[0]):\n",
    "  df.loc[row, 'cleaned_tweet'] =  ' '.join (custom_cleaner (col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @AtedoPeterside: Nobody \"says\" it better than our GoAmbassador @mrmacaronii. Take matters into your own hands by acquiring your PVC now.…</td>\n",
       "      <td>nobody says it better than our goambassador mrmacaronii take matters into your own hands by acquiring your pvc now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @1dernet: Getting Our PVC And Registering To Vote Come 2023 Is The Nonviolent Part To Take.\\n#GetYourPVC \\n@DeleFarotimi \\n@Tsngcampaign ht…</td>\n",
       "      <td>getting our pvc and registering to vote come is the nonviolent part to take delefarotimi tsngcampaign ht</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @Chude__: Hear what late Dora Akunyeli said about Peter Obi.. \\n\\n#GetYourPVC\\n#Competense2023\\n#peterObi2023 https://t.co/L0u1FqY4dB</td>\n",
       "      <td>hear what late dora akunyeli said about peter obi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment  \\\n",
       "0  Positive   \n",
       "1   Neutral   \n",
       "2   Neutral   \n",
       "\n",
       "                                                                                                                                              text  \\\n",
       "0     RT @AtedoPeterside: Nobody \"says\" it better than our GoAmbassador @mrmacaronii. Take matters into your own hands by acquiring your PVC now.…   \n",
       "1  RT @1dernet: Getting Our PVC And Registering To Vote Come 2023 Is The Nonviolent Part To Take.\\n#GetYourPVC \\n@DeleFarotimi \\n@Tsngcampaign ht…   \n",
       "2         RT @Chude__: Hear what late Dora Akunyeli said about Peter Obi.. \\n\\n#GetYourPVC\\n#Competense2023\\n#peterObi2023 https://t.co/L0u1FqY4dB   \n",
       "\n",
       "                                                                                                        cleaned_tweet  \n",
       "0  nobody says it better than our goambassador mrmacaronii take matters into your own hands by acquiring your pvc now  \n",
       "1            getting our pvc and registering to vote come is the nonviolent part to take delefarotimi tsngcampaign ht  \n",
       "2                                                                   hear what late dora akunyeli said about peter obi  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities = [get_relation(i) for i in tqdm(df['text'])]\n",
    "reps = []\n",
    "for text in df ['cleaned_tweet'] [0:3]:\n",
    "  sentences = nltk.sent_tokenize(text)\n",
    "  rep = []\n",
    "  for sentence in sentences:\n",
    "    try: \n",
    "      rep.append ({'entities': get_entities (sentence), 'relation': get_relation (sentence)})\n",
    "    except (IndexError):\n",
    "      pass\n",
    "\n",
    "  reps.append (rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[30m██████████\u001b[0m| 11659/11659 [02:58<00:00, 65.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# entities = [get_relation(i) for i in tqdm(df['text'])]\n",
    "reps = []\n",
    "\n",
    "for text in tqdm (df ['cleaned_tweet'], total=df.shape[0]):\n",
    "  # sentences = nltk.sent_tokenize(text)\n",
    "  rep = {'entities': [], 'relation': []}\n",
    "  # for sentence in sentences:\n",
    "  try: \n",
    "    # {'entities': [*rep['entities'], *get_entities (sentence)], 'relation': [*rep['relation'], get_relation (sentence)]}\n",
    "    rep = {'entities': get_entities (text), 'relation': get_relation (text)}\n",
    "  except (IndexError):\n",
    "    pass\n",
    "\n",
    "  reps.append (rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[30m██████████\u001b[0m| 11659/11659 [06:07<00:00, 31.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# pip3 install neo4j-driver\n",
    "# python3 example.py\n",
    "driver = GraphDatabase.driver(\n",
    "  \"bolt://localhost:7687\",\n",
    "  auth=basic_auth(\"neo4j\", password=\"12345\"))\n",
    "\n",
    "cypher_query = '''\n",
    "MERGE (n:Subject {name:$word_1})\n",
    "  ON CREATE SET n.count = 1\n",
    "  ON MATCH SET n.count = n.count + 1  \n",
    "MERGE (m:Object {name:$word_2})\n",
    "  ON CREATE SET m.count = 1\n",
    "  ON MATCH SET m.count = m.count + 1\n",
    "MERGE (n) - [r:VERB {name: $verb}] -> (m)\n",
    "  ON CREATE SET r.count = 1\n",
    "  ON MATCH SET r.count = r.count + 1\n",
    "''' \n",
    "\n",
    "with driver.session (database=\"neo4j\") as session:\n",
    "\n",
    "  for i in tqdm (range (len (reps)), total=len (reps)):\n",
    "    rel = reps[i]['relation']\n",
    "    try:\n",
    "      session.write_transaction (\n",
    "        lambda tx: tx.run (\n",
    "          cypher_query,\n",
    "          parameters={\"word_1\": reps[i]['entities'][0], \"word_2\": reps[i]['entities'][1], 'verb':rel})\n",
    "      )\n",
    "\n",
    "    except:\n",
    "      pass\n",
    "  \n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f571141c76525133f3e4c7bf9d670f19b99725fb293016e32e1a5a61960e7f7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
